{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486c5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a61e1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the components into the complete model\n",
    "class LandCoverModel(nn.Module):\n",
    "    def __init__(self, backbone, weakly_supervised_module):\n",
    "        super(LandCoverModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.weakly_supervised_module = weakly_supervised_module\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        logits = self.weakly_supervised_module(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03e17abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "data_dir = 'Feb_17_2024_workspace/Tile256/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a65028fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torchvision's ImageFolder to load the dataset\n",
    "your_dataset = ImageFolder(root=data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "366ade51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets (adjust as needed)\n",
    "train_size = int(0.8 * len(your_dataset))\n",
    "val_size = len(your_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(your_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "969b1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "import torch\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Resize images to a fixed size\n",
    "    batch = [(torch.nn.functional.interpolate(image.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False)).squeeze(0) for image in batch]\n",
    "    return torch.stack(batch, dim=0)\n",
    "\n",
    "# Update your DataLoader to use the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2985bec",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Resize images to a fixed size\n",
    "    batch = [(torch.nn.functional.interpolate(image[0].unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False)).squeeze(0) for image in batch]\n",
    "    return torch.stack(batch, dim=0)\n",
    "\n",
    "# Update your DataLoader to use the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a4da821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Resize images to a fixed size\n",
    "    resized_images = [(torch.nn.functional.interpolate(image.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False)).squeeze(0) for image, _ in batch]\n",
    "    labels = [label for _, label in batch]\n",
    "    return torch.stack(resized_images, dim=0), torch.tensor(labels)\n",
    "\n",
    "# Update your DataLoader to use the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7c07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the components\n",
    "backbone = ResolutionPreservingBackbone()\n",
    "weakly_supervised_module = WeaklySupervisedModule(num_classes=len(your_dataset.classes))\n",
    "contrastive_loss_fn = ContrastiveLoss()\n",
    "model = LandCoverModel(backbone, weakly_supervised_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c84d9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and training parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572271b",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        \n",
    "        # Example weakly supervised loss (adjust as needed)\n",
    "        weak_loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        # Example self-supervised contrastive loss\n",
    "        contrastive_loss = contrastive_loss_fn(logits)\n",
    "        \n",
    "        # Combine the losses based on your specific strategy\n",
    "        total_loss = weak_loss + contrastive_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation (optional)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_logits = model(val_images)\n",
    "            # Validation logic, if needed\n",
    "\n",
    "    # Print or log training statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c866f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "TRAINING MODEL 2\n",
    "\"\"\"\n",
    "# Training loop...................model2.........2......Training....\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        \n",
    "        # Example weakly supervised loss (adjust as needed)\n",
    "        weak_loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "        # Example self-supervised contrastive loss\n",
    "        contrastive_loss = contrastive_loss_fn(logits)\n",
    "\n",
    "        # Combine the losses based on your specific strategy\n",
    "        total_loss = weak_loss + contrastive_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation (optional)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_logits = model(val_images)\n",
    "            # Validation logic, if needed\n",
    "\n",
    "    # Print or log training statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item()}')\n",
    "\n",
    "# Save or use the trained model for land-cover mapping\n",
    "torch.save(model.state_dict(), 'land_cover_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f726ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#..................add()..................METRICES..................\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, cohen_kappa_score\n",
    "\n",
    "\"\"\"\"\n",
    "# Define the LandCoverModel, ResolutionPreservingBackbone, WeaklySupervisedModule, \n",
    "# and ContrastiveLoss as in the previous example\n",
    "\"\"\"\n",
    "\n",
    "# Function to calculate Intersection over Union (IoU)\n",
    "def calculate_iou(y_true, y_pred):\n",
    "    intersection = np.logical_and(y_true, y_pred)\n",
    "    union = np.logical_or(y_true, y_pred)\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "    return iou\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            logits = model(val_images)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_labels.extend(val_labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, save_path):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(save_path, dpi=1000)\n",
    "    plt.close()\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "THE COMPLETE TRAINING AND PRODUCING ALL METRICES TO EVALUATE MODEL PERFORMANCE\n",
    "\"\"\"\n",
    "# Training loop with evaluation and plotting\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = your_loss_function(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_labels, val_predictions = evaluate_model(model, val_loader, device)\n",
    "\n",
    "    # Calculate metrics\n",
    "    iou = calculate_iou(val_labels, val_predictions)\n",
    "    f1 = f1_score(val_labels, val_predictions, average='weighted')\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    kappa = cohen_kappa_score(val_labels, val_predictions)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], IoU: {iou:.4f}, F1-Score: {f1:.4f}, Accuracy: {accuracy:.4f}, Kappa: {kappa:.4f}')\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    plot_confusion_matrix(val_labels, val_predictions, your_class_labels, f'confusion_matrix_epoch_{epoch+1}.png')\n",
    "\n",
    "# Save or use the trained model for land-cover mapping\n",
    "torch.save(model.state_dict(), 'land_cover_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
