{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57417037",
   "metadata": {},
   "source": [
    "# OVERALL ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, you can extract accuracy from the history object\n",
    "final_accuracy = history1.history['accuracy'][-1]\n",
    "print(\"Final Accuracy:\", final_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020c3d9",
   "metadata": {},
   "source": [
    "# TRAINING BEHAVIOUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, you can extract accuracy from the history object\n",
    "final_accuracy = history1.history['accuracy'][-1]\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bx', label='Training loss')\n",
    "plt.plot(epochs, val_loss, '-rx', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history1.history['iou_score']\n",
    "val_acc = history1.history['val_iou_score']\n",
    "\n",
    "plt.plot(epochs, acc, '-bx', label='Training IOU')\n",
    "plt.plot(epochs, val_acc, '-rx', label='Validation IOU')\n",
    "plt.title('Training and validation IOU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IOU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdb65d",
   "metadata": {},
   "source": [
    "# General confusion matrix-the overall predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Assuming you have your model `model` and test data `X_test` already defined and trained\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Convert predictions and ground truth masks back to label format\n",
    "y_pred_labels = np.argmax(y_pred, axis=-1)\n",
    "y_test_labels = np.argmax(y_test_cat, axis=-1)\n",
    "\n",
    "# Flatten the labels for classification report and confusion matrix\n",
    "y_pred_flat = y_pred_labels.flatten()\n",
    "y_test_flat = y_test_labels.flatten()\n",
    "\n",
    "# Calculate classification report\n",
    "report = classification_report(y_test_flat, y_pred_flat)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = accuracy_score(y_test_flat, y_pred_flat)\n",
    "print(\"Overall Accuracy:\", accuracy)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=range(cm.shape[1]), yticklabels=range(cm.shape[0]))\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Save the plot as a TIFF image with 1000 DPI\n",
    "plt.savefig('confusion_matrix.png', dpi=1000)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14425da6",
   "metadata": {},
   "source": [
    "# Confusion matrix of each class, individual-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have your model `model` and test data `X_test` already defined and trained\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Convert predictions and ground truth masks back to label format\n",
    "y_pred_labels = np.argmax(y_pred, axis=-1)\n",
    "y_test_labels = np.argmax(y_test_cat, axis=-1)\n",
    "\n",
    "# Get unique labels\n",
    "labels = np.unique(y_test_labels)\n",
    "\n",
    "# Calculate confusion matrix for each class\n",
    "for label in labels:\n",
    "    # Filter predictions and ground truth for the current label\n",
    "    y_pred_label = y_pred_labels[y_test_labels == label]\n",
    "    y_true_label = y_test_labels[y_test_labels == label]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true_label, y_pred_label)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title(f'Confusion Matrix - Class {label}')\n",
    "    \n",
    "    # Save the plot as a TIFF image with 1000 DPI\n",
    "    plt.savefig(f'confusion_matrix_class_{label}.tif', dpi=1000)\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c66ea0",
   "metadata": {},
   "source": [
    "# Accuracy of each class in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your model `model` and test data `X_test` already defined and trained\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Convert predictions and ground truth masks back to label format\n",
    "y_pred_labels = np.argmax(y_pred, axis=-1)\n",
    "y_test_labels = np.argmax(y_test_cat, axis=-1)\n",
    "\n",
    "# Get unique labels\n",
    "labels = np.unique(y_test_labels)\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "for label in labels:\n",
    "    # Filter predictions and ground truth for the current label\n",
    "    y_pred_label = y_pred_labels[y_test_labels == label]\n",
    "    y_true_label = y_test_labels[y_test_labels == label]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_true_label, y_pred_label)\n",
    "    \n",
    "    # Print accuracy for the current class\n",
    "    print(f'Accuracy - Class {label}: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9402a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
